## Get hostname with the following command: 
## $ hostname -f
##
## Configure an environment per hostname:
## [hostname1]
## ...
##
## Use the same environment for multiple hostnames:
## [hostname2, hostname3, ...]
## ...
##
## Using group
## [hostname1, hostname2, ... : group]
## [group]
## ...
##
## Using an asterisk in hostnames (IMPORTANT: only one * is allowed in hostnames)
##
## [host*name1]
##
## [*hostname2, hostname3*]



# Stanford Kundaje group clusters (out of SGE)
[vayu, mitra, durga]
conda_env	= aquas_chipseq
conda_env_py3	= aquas_chipseq_py3
conda_bin_dir   = /software/miniconda3/bin
species_file	= $script_dir/species/kundaje.conf
unlimited_mem_wt= true 		# unlimited max. memory and walltime on Kundaje clusters
nice            = 10
nth             = 4


# Stanford Kundaje group clusters (controlled with SGE)
[nandi, kali, amold, wotan, kadru, surya, indra]
conda_env	= aquas_chipseq
conda_env_py3	= aquas_chipseq_py3
conda_bin_dir   = /software/miniconda3/bin
species_file	= $script_dir/species/kundaje.conf
unlimited_mem_wt= true 		# unlimited max. memory and walltime on Kundaje clusters
system 		= sge 		# force to use SGE (Sun Grid Engine)
nice            = 20
nth             = 4


# Stanford SCG : login node, computing nodes, file transfer servers
[scg*.stanford.edu, scg*.local, carmack.stanford.edu, crick.stanford.edu]
conda_env	= aquas_chipseq
conda_env_py3	= aquas_chipseq_py3
species_file	= $script_dir/species/scg.conf
nth 		= 8		# number of threads for each pipeline run
wt_spp 		= 72h		# walltime for spp
system 		= sge 		# force to use SGE (Sun Grid Engine) on SCG3/4 even though a user doesn't explicitly specify SGE on command line with 'bds -s sge chipseq.bds ...'
retrial 	= 1 		# number of retrial for failed tasks (SCG queue master kills tasks very frequently according to node's available resources)


# Stanford Sherlock clusters 
[sherlock*.stanford.edu, sh-*.local]
conda_env	= aquas_chipseq
conda_env_py3	= aquas_chipseq_py3
species_file	= $script_dir/species/sherlock.conf
nth		= 8		# number of threads for each pipeline run
wt_spp		= 47h		# walltime for spp
system		= slurm		# force to use SLURM
retrial		= 1 		# number of retrial for failed tasks


# default (if no section with hostname is found)
[default]
conda_env	= aquas_chipseq
conda_env_py3	= aquas_chipseq_py3
species_file	= # use your own species file here. (DEF_SPECIES_FILE: DO NOT REMOVE THIS COMMENT!)



