BigDataScript (BDS) pipelines using AQUAS pipeline modules
==============


## Managing multiple pipelines

`./utils/bds_scr` is a BASH script to create a detached screen for a BDS script and redirect stdout/stderr to a log file `[LOG_FILE_NAME]`. If a log file already exists, stdout/stderr will be appended to it. Monitor a pipeline with `tail -f [LOG_FILE_NAME]`.

The only difference between `bds_scr` and `bds` is that you have `[SCR_NAME] [LOG_FILE_NAME]` between `bds` command and its parameters (or a BDS script name).
You can skip `[LOG_FILE_NAME]` then a log file `[SCR_NAME].log` will be generated on the working directory.
You can also add any BDS parameters (like `-dryRun`, `-d` and `-s`). The following example is for running a pipeline on Sun Grid Engine.
Use `bds_scr_5min` instead of `bds_scr` to prevent from running multiple pipelines on the same data set and output directory. `bds_scr_5min` does not start a screen if the last modified time of a log file is fresh (5 minutes).

```
$ bds_scr [SCR_NAME] [LOG_FILE_NAME] [PIPELINE.BDS] ...
$ bds_scr [SCR_NAME] [PIPELINE.BDS] ...
$ bds_scr [SCR_NAME] [LOG_FILE_NAME] -s sge [PIPELINE.BDS] ...
$ bds_scr_5min [SCR_NAME] [PIPELINE.BDS] ...
```

Once the pipeline run is done, the screen will be automatically closed. To kill a pipeline manually while it's running:
```
$ kill_scr [SCR_NAME]
$ screen -X -S [SCR_NAME] quit
```

## Specifying a cluster engine

You can run BDS pipeline with a specified cluster engine. Choose your cluster system (`local`: UNIX threads, `sge`: Sun Grid Engine, `slurm`: SLURM ...).
```
$ bds -s [SYSTEM] [PIPELINE.BDS] ...
```

Modify `$HOME/.bds/bds.config` or `./default.env` to change your default system. The following example is to use Sun Grid Engine (sge) as your default system. Then you no longer need to add `-s sge` to the command line.
```
#system = local
system = sge
#system = slurm
```

You need additional modification on `bds.config` to correctly configure your cluster engine. Read more at [here](http://pcingola.github.io/BigDataScript/bigDataScript_manual.html).

For Kundaje lab clusters, SCG and Sherlock clusters, it's already set up for Sun Grid Engine and SLURM. SLURM is implemented with using a generic cluster (`system = generic`) for AQUAS pipeline modules.

## Resource settings

Most clusters have resource limitation so that jobs submitted without it will be declined. By default, walltime is 23 hours and max memory is 12GB. To change them, add the following parameters to the command line. `-mem` does not apply to jobs with their own max. memory parameters (eg. `-mem_spp` for spp, `-mem_bwa` for bwa, ...)
```
-wt [WALLTIME; examples: 5:50:00, 10h20m, 7200] -memory [MAX_MEMORY; examples: 5G, 2000K]
```
You can specify walltime and max. memory for a specific job (with `-mem_[APP_NAME] [MAX_MEM]`). To see which job has specific resource settings, run the pipeline without parameters `$ bds [PIPELINE_BDS]` then it will display all parameters including resource settings and help. The following line is an example parameter to increase walltime and max. memory for MACS2 peak calling.
```
-wt_macs2 10h30m -mem_macs2 15G
```
Note that max. memory defined with `-mem_XXX` is NOT PER CPU! If your system (either local or cluster engine) doesn't limit walltime and max. memory for jobs, add the following to the command line. Pipeline jobs will run without resource restriction.
```
-unlimited_mem_wt
```

## Debugging BDS pipelines

1. Take a look at HTML report (which contains all STDERR/STDOUT for all jobs in the pipeline; ).  It tells you everything about all pipeline jobs. Find which stage is errorneous. Carefully look at system messages (STDERR and STDOUT) for it. BDS HTML report is located at the working folder with name `[PIPELINE_NAME]_[TIMESTAMP]_report.html`. This report is automatically generated by BDS.

2. Correct errors.

   2.1. Lack of memory: increase memory for all jobs (e.g. add `-mem 20G`) or a specific problematic job (e.g. add `-mem_macs2 20G`).
   
   2.2. Timeout: increase walltime for all jobs (e.g. add `-wt 24h`) or a specific long job (e.g. add `-wt_macs2 200h`). (Warning! Most clusters have limit for walltime. Make it as shortest as you can to get your queued jobs executed quickly.)
   
   2.3. Wrong input: check all input files are available.
   
   2.4. Software error: use recommended software versions.


3. Resume pipeline with the same command line that you used for starting it. Previous successful stages will be automatically skipped.

```
# make BDS verbose
$ bds -v [PIPELINE.BDS] ...

# display debugging information
$ bds -d [PIPELINE.BDS] ...

# test run (this actually does nothing) to check input/output file names and commands
$ bds -dryRun [PIPELINE.BDS] ...
```

## Species file

There are many species specific parameters like indices (bwa, bowtie, ...), chromosome sizes and sequence files (chr*.fa). If you have multiple pipelines, it's inconvenient to individually define all parameters in a command line argument for each pipeline run. However, if you have a species file with all species specific parameters defined, then you define less parameters in the command line and share the species file with all other pipelines.

Add the following to the command line to specify species and species file.

```
-species [SPECIES; hg19, mm9, ...] -species_file [SPECIES_FILE]
```

You can override any parameters defined in the species file by adding them to command line argument or configuration file. For example, if you want to override parameters for BWA index and umap:

```
-species hg19 -species_file my_species.conf -bwa_idx [YOUR_OWN_BWA_IDX] -chrsz [YOUR_OWN_CHR_SIZES_FILE]
```

Example species file looks like the following. You can define your own species.

```
[hg19]
chrsz   = /mnt/data/annotations/by_release/hg19.GRCh37/hg19.chrom.sizes # chromosome sizes
seq     = /mnt/data/ENCODE/sequence/encodeHg19Male 			# genome reference sequence
gensz   = hs 								# genome size: hs for humna, mm for mouse
umap    = /mnt/data/ENCODE/umap/encodeHg19Male/globalmap_k1tok1000 	# uniq. mappability tracks
bwa_idx = /mnt/data/annotations/indexes/bwa_indexes/encodeHg19Male/v0.7.10/encodeHg19Male_bwa-0.7.10.fa
blacklist = /mnt/data/ENCODE/blacklists/wgEncodeDacMapabilityConsensusExcludable.bed.gz

# added for for atac-seq pipeline (bds_atac/atac.bds)
bwt2_idx = /mnt/data/annotations/indexes/bowtie2_indexes/bowtie2/ENCODEHg19_male

# added for ATAQC
tss_enrich = /mnt/lab_data/kundaje/users/dskim89/ataqc/annotations/hg19/hg19_RefSeq_stranded.bed.gz
ref_fa  = /mnt/lab_data/kundaje/users/dskim89/ataqc/annotations/hg19/encodeHg19Male.fa  // genome reference fasta
dnase = /mnt/lab_data/kundaje/users/dskim89/ataqc/annotations/hg19/reg2map_honeybadger2_dnase_all_p10_ucsc.bed.gz
prom = /mnt/lab_data/kundaje/users/dskim89/ataqc/annotations/hg19/reg2map_honeybadger2_dnase_prom_p2.bed.gz
enh = /mnt/lab_data/kundaje/users/dskim89/ataqc/annotations/hg19/reg2map_honeybadger2_dnase_enh_p2.bed.gz
reg2map = /mnt/lab_data/kundaje/users/dskim89/ataqc/annotations/hg19/dnase_avgs_reg2map_p10_merged_named.pvals.gz
roadmap_meta = /mnt/lab_data/kundaje/users/dskim89/ataqc/annotations/hg19/eid_to_mnemonic.txt

# your own definition for species

[hg19_custom]
chrsz   = ...
seq     = ...
...

[mm9]
...

[mm10]
...
```

Description for parameters in a species file.
```
chrsz               : Chromosome sizes file path (use fetchChromSizes from UCSC tools).
seq                 : Reference genome sequence directory path (where chr*.fa exist).
gensz               : Genome size; hs for human, mm for mouse (default: hs).
umap                : Unique mappability tracks directory path.
bwa_idx             : BWA index (full path prefix of [].bwt file) .

# for atac-seq pipeline (atac.bds)
bwt2_idx            : Bowtie2 index (full path prefix of [].1.bt2 file).

# for ATAQC
tss_enrich          : TSS enrichment bed.
ref_fa 		    : Reference genome sequence fasta.
blacklist 	    : Blacklist bed for ataqc.
dnase 		    : DNase bed for ataqc.
prom 		    : Promoter bed for ataqc.
enh 		    : Enhancer bed for ataqc.
reg2map 	    : Reg2map for ataqc.
roadmap_meta 	    : Roadmap metadata for ataqc.
```
Unique mappability tracks areavaiable [here](https://sites.google.com/site/anshulkundaje/projects/mappability). Blacklists are available [here](https://sites.google.com/site/anshulkundaje/projects/blacklists).

## Sharing a species file

```
$ bds [PIPELINE_BDS] -species [SPECIES; hg19, mm9, ...] -species_file [SPECIES_FILE]
```
If you want to skip `-species_file` parameter, define it in the default environment file `./default.env`.
```
[your_hostname] # get it with 'hostname -f'

conda_env     = [CONDA_ENV_NAME; bds_atac for atac, aquas_chipseq for chipseq]
conda_env_py3 = [CONDA_ENV_PY3_NAME; bds_atac_py3 for atac, aquas_chipseq_py3 for chipseq]

species_file = [SPECIES_FILE]
```

## Setting up shell environment

**Ignore this section** if you have installed dependencies with `./install_dependencies.sh`.

It is important to define enviroment variables (like `$PATH`) to make bioinformatics softwares in the pipeline work properly. mod, shcmd and addpath are three convenient ways to define environment variables. Environment variables defined with mod, shcmd and addpath are preloaded for all tasks on the pipeline. For example, if you define environment variables for bwa/0.7.3 with mod. bwa of version 0.7.3 will be used throughout the whole pipeline (including bwa aln, bwa same and bwa sampe).

1) `mod`

   There are different versions of bioinformatics softwares (eg. samtools, bedtools and bwa) and <a href="http://modules.sourceforge.net/">Enviroment Modules</a> is the best way to manage environemt variables for them. For example, if you want to add environment variables for bwa 0.7.3 by using Environment Modules. You can simply type `$ module add bwa/0.7.3;`. The equivalent setting in the pipeline configuration file should look like:

       mod= bwa/0.7.3;

   You can have multiple lines for mod since any suffix is allowed. Use ` ` as a delimiter.

       mod_bio= bwa/0.7.3 bedtools/2.x.x samtools/1.2
       mod_lang= r/3.2.2 java/latest

2) `shcmd`

   If you have softwares locally installed on your home, you may need to add to them environment variables like `$PATH`, `$LD_LIBRARY_PATH` and so on. <b>IMPORTANT!</b> Note that any pre-defined enviroment variables (like `$PATH`) should be referred in a curly bracket like `${PATH}`. This is because BDS distinguishes environment variables from BDS variables by a curly bracket ${}.

       shcmd= export PATH=${PATH}:path_to_your_program

   You can have multiple lines for shcmd since any suffix is allowed. Use ; as a delimiter. 

       shcmd_R= export PATH=${PATH}:/home/userid/R-3.2.2;
       shcmd_lib= export LD_LIBRARY_PATH=${LD_LIBRARY_PATH}:${HOME}/R-3.2.2/lib

   shcmd is not just for adding environemt variables. It can execute any bash shell commands prior to any jobs on the pipeline. For example, to give all jobs peaceful 10 seconds before running.

       shcmd_SLEEP_TEN_SECS_FOR_ALL_JOBS= echo "I am sleeping..."; sleep 10

3) `addpath`

   If you just want to add something to your `$PATH`, use addpath instead of shcmd. It's much simpler. Use : or ; as a delimiter.

       addpath= ${HOME}/program1/bin:${HOME}/program1/bin:${HOME}/program2/bin:/usr/bin/test

4) `conda_env` and `conda_env_py3`

   You can also use Anaconda virtual environment in the pipeline. BDS pipelines usually take two conda environments since there is a conflict between softwares based on python3 and python2. Make sure that the environment corresponding to `conda_env` has python2 installed and that corresponding to `conda_env_py3` has python3 installed.

       conda_env= [CONDA_ENV_NAME]		# python2 must be installed in this virt. env.
       conda_env_py3= [CONDA_ENV_NAME_FOR_PY3] 	# python3 must be installed in this virt. env.

`-mod`, `-shcmd` and `-addpath` are command line argument versions of `mod`, `shcmd` and `addpath`. However NO SUFFIX is allowed. For example,

```
$ bds [PIPELINE_BDS] -mod 'bwa/0.7.3; samtools/1.2' -shcmd 'export PATH=${PATH}:/home/userid/R-3.2.2' -addpath '${HOME}/program1/bin' -conda_env my_env -conda_env_py3 my_env_py3
```

## Environment file

It should be more convenient to have a separate file to define your own shell environments and cluster resources per hostname. You can also define any parameters (like bwa index, # thread for tasks, fastqs and so on) in the environment file. If an environment file is not specified `./default.env` will be used by default.

```
$ bds [PIPELINE_BDS] ... -env [ENV_FILE]
```

Shell environment settings can be set up per HOSTNAME (`$ hostname -f`). This means that you can have multiple environment configruation for all clusters in one environment file. Single or multiple hostnames are written in a square bracket ([SECTION_NAME]). You can also use a group for hostnames. You can use a wild card (**only one asterisk!**) in hostnames. Example sturcture is like the following:

```
[hostname1]
... parameters ...

[hostname2]
... parameters ...

[hostname3, hostname4]
... parameters ...


[hostname5 : group1]
[hostname6, hostname7: group2]

[group1]
... parameters ...

[group2]
... parameters ...
```

Example environment file is like the following. Take a look at `./default.env`.
```
[sherlock*.stanford.edu, sh-*.local] 	# your hostname

mod_any_suffix = bwa/0.7.3 samtools/1.2
addpath_any_suffix = ${HOME}/program1/bin
shcmd_any_suffix = export R_PATH=/home/userid/R-3.2.2

species_file = /path/to/your/species.conf

mem_spp = 4G
wt_spp  = 10:00:00
nice 	= 19 		# sub tasks will have low priority (nice==19).
system 	= slurm 	# SLURM

conda_env = my_conda_env_py2
conda_env_py3 = my_conda_env_py3
...

[other_host_name_or_group]
...

```

Parameters can be defined in 1) environment file, 2) configuation file and 3) command line arguments. They will be overriden in the order of 1) < 2) < 3).

## Setting up Sun Grid Engine

Add the following to grid engine configuration.
```
$ sudo qconf -mconf
...
execd_params                 ENABLE_ADDGRP_KILL=true
...
```

Add a parallel environment `shm` to grid engine configuration. If you already have your own parallel environment, skip this.
```
$ sudo qconf -ap

pe_name            shm
slots              999
user_lists         NONE
xuser_lists        NONE
start_proc_args    /bin/true
stop_proc_args     /bin/true
allocation_rule    $pe_slots
control_slaves     FALSE
job_is_first_task  FALSE
urgency_slots      min
accounting_summary FALSE
```

Add your parallel environment (`shm` by default) to your queue and set your shell as bash.
```
$ sudo qconf -mq [YOUR_MAIN_QUEUE]
...
pe_list               make [YOUR_PE_NAME]
shell                 /bin/bash
...
```

Correctly configure `./bds.config` and copy it to `$HOME/.bds/`:
```
sge.pe = [YOUR_PE_NAME] # shm by default
sge.mem = [MAX_MEMORY_TYPE] # h_vmem by default
sge.timeout = [HARD_WALLTIME_TYPE] # h_rt by default
sge.timeout2 = [SOFT_WALLTIME_TYPE] # s_rt by default
```

More information is at [here](http://pcingola.github.io/BigDataScript/bigDataScript_manual.html">http://pcingola.github.io/BigDataScript/bigDataScript_manual.html).

## BASH completion for UNIX screens

For automatic BASH completion for screens (http://www.commandlinefu.com/commands/view/12160/bash-auto-complete-your-screen-sessions), add the following to your `$HOME/.bashrc`:
```
complete -C "perl -e '@w=split(/ /,\$ENV{COMP_LINE},-1);\$w=pop(@w);for(qx(screen -ls)){print qq/\$1\n/ if (/^\s*\$w/&&/(\d+\.\w+)/||/\d+\.(\$w\w*)/)}'" screen
```

## Troubleshooting

### /bin/bash: module: line 1: syntax error: unexpected end of file

If see the following error when you submit jobs to Sun Grid Enginee,
```
/bin/bash: module: line 1: syntax error: unexpected end of file
```

Check if your $HOME/.bashrc has any errorneous lines.

Remove the following line in you module initialization scripts ($MODULESHOME/init/bash or /etc/profile.d/modules.sh).
```
export -f module
```

### Unable to run job: unknown resource "'mem"

Replace `$HOME/.bds/bds.config` with the one in the repo.
```
$ cp /path/to/repo/bds.config $HOME/.bds/
```

### Unable to access jarfile /picard.jar

Define a shell variable `PICARDROOT` for your environment. Add the following to your `~/.bashrc` or conda `activate`:
```
export PICARDROOT=/path/to/your/picard-tool
```

### awk: cmd. line:1: fatal: division by zero attempted

This error happens when 1) picard tool's MarkDuplicate is running out of memory, 2) fastq inputs have wrong endedness (SE or PE) or 3) input raw bam is incorrect.
For 1) balance memory usage among parallel tasks, add `-no_par` or reduce max. # threads (`-nth [SMALL_NUMBER]`).
For 2) check your fastq inputs are correct (`-fastqN_1`, `-fastqN_2`, ...) and also check their endedness (SE or PE) parameters like (`-se` or `-pe`).
For 3) check if there is an error in aligning stage (in an HTML report).


### Unsupported major.minor version (java)

When running bds (BigDataScript), you get the following error if you have lower version of java or high version of java is not selected as default.

Solution:
```
# install latest version of java

# for Fedora based linux (Red Hat, ...)
$ sudo apt-get install openjdk-8-jre

# fr Debian based linux (Ubuntu, ...)
$ sudo yum install java-1.8.0-openjdk

# choose the latest java as default
$ sudo update-alternatives --config java
```


### [main_samview] random alignment retrieval only works for indexed BAM or CRAM files.

If your pipeline starts from BAM files, make sure that bam index (.bam.bai) exists together with BAM file. If not, build index with `samtools index [YOUR_BAM_FILE]`. BAM and BAI should be in the same directory.


### Fatal error: /home/leepc12/bds_atac/modules/report_*.bds

Simply re-run the pipeline with the same command. Possible bug in BDS for locking/unlocking global variables.


### ImportError: libopenblasp-r0-39a31c03.2.18.so: cannot open shared object file: No such file or directory

Dependencies are not installed correctly. Check your Anaconda Python is correctly configured for conda environments. Run `./uninstall_dependencies.sh` and then `./install_dependencies.sh` again.


### Unable to run job: unknown resource "mem"

Copy `./bds.config` to `$HOME/.bds/`.


### Error trying to find out post-mortem info on task

For fast scheduling clusters including SGE, doing post-mortem on jobs can fail in BDS. Add `clusterPostMortemDisabled = true` to your `~/.bds/bds.config`.


### java.lang.OutOfMemoryError: unable to create new native thread

Number of threads created by BDS exceeds limit (`ulimit -a`). BDS created lots of thread per pipeline (more than 20). So if you see any thread related error, check your `ulimit -a` and increase it a bit.


### Task disappeared

Check a log of failed tasks with `qacct -j [JOB_ID]` (for SGE). You job can be killed due to resource settings. Increase your walltime or max. memory (`-wt`, `-wt_APPNAME`, `-mem` or `-mem_APPNAME`) for the task of failure.


### File exists, No file or directory (related to parallel conda activations)

This is a known bug in conda [#2837](https://github.com/conda/conda/issues/2837) and has not been fixed yet even in the latest version (4.2.1). Downgrade conda to 4.0.5 or 4.0.10. 

```
Traceback (most recent call last):
  File "/home/leepc12/miniconda3/bin/conda", line 6, in <module>
    sys.exit(main())
  File "/home/leepc12/miniconda3/lib/python3.5/site-packages/conda/cli/main.py", line 48, in main
    activate.main()
  File "/home/leepc12/miniconda3/lib/python3.5/site-packages/conda/cli/activate.py", line 135, in main
    conda.install.symlink_conda(prefix, root_dir, shell)
  File "/home/leepc12/miniconda3/lib/python3.5/site-packages/conda/install.py", line 596, in symlink_conda
    symlink_conda_hlp(prefix, root_dir, where, symlink_fn)
  File "/home/leepc12/miniconda3/lib/python3.5/site-packages/conda/install.py", line 613, in symlink_conda_hlp
    symlink_fn(root_file, prefix_file)
FileExistsError: [Errno 17] File exists: '/home/leepc12/miniconda3/bin/conda' -> '/home/leepc12/miniconda3/envs/bds_atac/bin/conda'

Traceback (most recent call last):
  File "/home/leepc12/miniconda3/bin/conda", line 6, in <module>
    sys.exit(main())
  File "/home/leepc12/miniconda3/lib/python3.5/site-packages/conda/cli/main.py", line 48, in main
    activate.main()
  File "/home/leepc12/miniconda3/lib/python3.5/site-packages/conda/cli/activate.py", line 135, in main
    conda.install.symlink_conda(prefix, root_dir, shell)
  File "/home/leepc12/miniconda3/lib/python3.5/site-packages/conda/install.py", line 596, in symlink_conda
    symlink_conda_hlp(prefix, root_dir, where, symlink_fn)
  File "/home/leepc12/miniconda3/lib/python3.5/site-packages/conda/install.py", line 610, in symlink_conda_hlp
    os.remove(prefix_file)
FileNotFoundError: [Errno 2] No such file or directory: '/home/leepc12/miniconda3/envs/bds_atac_py3/bin/conda'
```

### Exception in thread "main" java.lang.NumberFormatException: For input string: "40G"

Do not use `-mem` in your command line. Use '-memory` instead.

```
$ bds chipseq.bds -mem 40G
Picked up _JAVA_OPTIONS: -Xms256M -Xmx1024M -XX:ParallelGCThreads=1
Exception in thread "main" java.lang.NumberFormatException: For input string: "40G"
        at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
        at java.lang.Long.parseLong(Long.java:589)
        at java.lang.Long.parseLong(Long.java:631)
        at org.bds.lang.Type.parse(Type.java:334)
        at org.bds.BdsParseArgs.parseArgs(BdsParseArgs.java:207)
        at org.bds.BdsParseArgs.initializeArgs(BdsParseArgs.java:150)
        at org.bds.BdsParseArgs.initializeArgs(BdsParseArgs.java:106)
        at org.bds.BdsParseArgs.parse(BdsParseArgs.java:172)
        at org.bds.Bds.runCompile(Bds.java:872)
        at org.bds.Bds.run(Bds.java:815)
        at org.bds.Bds.main(Bds.java:182)
```

# Contributors

* Jin wook Lee - PhD Student, Mechanical Engineering Dept., Stanford University
* Anshul Kundaje - Assistant Professor, Dept. of Genetics, Stanford University
